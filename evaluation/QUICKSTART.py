"""
Quick Reference Guide - NID Evaluation Module
"""

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        NID EVALUATION MODULE - QUICK START                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ LOCATION: /evaluation/

ğŸ“‹ FILES:
  â€¢ evaluator.py     - Main evaluation engine
  â€¢ summary.py       - Statistical analysis and reporting
  â€¢ __init__.py      - Module interface
  â€¢ requirements.txt - Dependencies
  â€¢ README.md        - Full documentation
  
ğŸš€ QUICK START:

1. Run full evaluation:
   $ python evaluator.py
   
2. View summary statistics:
   $ python summary.py

3. Use as Python module:
   from evaluation import ResultsEvaluator
   evaluator = ResultsEvaluator(person1, person2, ground_truth)
   results = evaluator.generate_report('output.csv')

ğŸ“Š OUTPUT:
  â€¢ evaluation_results.csv - Detailed results with metrics
  â€¢ Summary statistics     - Printed to console
  â€¢ Quality assessment     - Accuracy distribution

ğŸ“ˆ METRICS EXPLAINED:

  ACCURACY (0-100%)
  â””â”€ Similarity ratio between entered vs ground truth
     â€¢ 100% = Perfect match
     â€¢ 90%+ = Excellent
     â€¢ 80%+ = Good

  CER - Character Error Rate (0-100%)
  â””â”€ Character-level differences
     â€¢ 0% = No errors
     â€¢ <5% = Excellent
     â€¢ <10% = Good

  WER - Word Error Rate (0-100%)
  â””â”€ Word-level differences
     â€¢ 0% = All words match
     â€¢ <20% = Good
     â€¢ >50% = Many word differences

ğŸ¯ QUALITY TIERS:
  â€¢ Excellent: â‰¥95% accuracy
  â€¢ Good: 80-95% accuracy
  â€¢ Fair: 60-80% accuracy
  â€¢ Poor: <60% accuracy

ğŸ“ RESULTS CSV STRUCTURE:

  Entered Data          Ground Truth           Metrics
  â”œâ”€ image_id          â”œâ”€ predicted_*_name    â”œâ”€ *_accuracy
  â”œâ”€ actual_english    â”œâ”€ predicted_*         â”œâ”€ *_cer
  â”œâ”€ actual_bangla     â””â”€ predicted_address   â”œâ”€ *_wer
  â”œâ”€ actual_father                            â””â”€ overall_*
  â”œâ”€ actual_mother
  â”œâ”€ actual_dob
  â”œâ”€ actual_nid_no
  â””â”€ actual_address

ğŸ” EXAMPLE USAGE IN CODE:

from evaluation import ResultsEvaluator, print_summary

# Initialize
evaluator = ResultsEvaluator(
    person1_csv='data/nid-data-entry-results-person1.csv',
    person2_csv='data/nid-data-entry-results-person2.csv',
    ground_truth_csv='data/nid-data-140126.csv'
)

# Run evaluation
results_df = evaluator.evaluate()
print(f"Matched records: {len(results_df)}")

# Generate report with stats
evaluator.generate_report('data/evaluation_results.csv')

# Print summary
print_summary('data/evaluation_results.csv')

ğŸ’¡ TIPS:

âœ“ Merge happens automatically when running evaluator.py
âœ“ Records matched by NID number if image IDs don't align
âœ“ Missing matches are logged as warnings
âœ“ All metrics are normalized to 0-100 scale
âœ“ Results CSV is comma-separated, quoted fields

âš ï¸  NOTES:

â€¢ Date formats must be consistent (YYYY-MM-DD)
â€¢ Names are case-insensitive for comparison
â€¢ Address comparison is word-level
â€¢ CER/WER may be 100% for numeric fields due to word splitting

ğŸ“ SUPPORT:

For issues or questions:
1. Check README.md for detailed documentation
2. Review evaluator.py source code for algorithm details
3. Check data format in evaluation_results.csv

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
